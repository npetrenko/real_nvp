{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from flows import NormalRW, DFlow, NVPFlow, LogNormal, GVAR, phase,\\\n",
    "Normal, floatX, MVNormal, MVNormalRW, Linear, LinearChol\n",
    "import flows\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.contrib.distributions import WishartCholesky\n",
    "import math\n",
    "from flows.models import VARmodel\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUS.csv  FRA.csv  GBR.csv  SYNTH.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls CDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccodes = ['FRA']#, 'FRA', 'GBR']\n",
    "datas = ['./CDATA/{}.csv'.format(x) for x in ccodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [pd.read_csv(x, index_col='VARIABLE').values.astype(floatX).T[np.newaxis][:,1:-1] for x in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 193, 4)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data.shape for data in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00571698 0.93158025 0.00503056 0.03016526]\n",
      "---\n",
      "[0.99999994 0.99999994 0.99999934 1.        ]\n"
     ]
    }
   ],
   "source": [
    "scaler = 0.\n",
    "for i, data in enumerate(datas):\n",
    "    stds = (data[0,1:] - data[0,:-1]).std(axis=0)\n",
    "    print(stds)\n",
    "    scaler = scaler + stds\n",
    "    datas[i] = data\n",
    "print('---')\n",
    "scaler /= len(datas)\n",
    "for i in range(len(datas)):\n",
    "    datas[i] /= scaler\n",
    "    data = datas[i]\n",
    "    stds = (data[0,1:] - data[0,:-1]).std(axis=0)\n",
    "    print(stds)\n",
    "    data = np.concatenate([data[:,1:], data[:,:-1]], axis=-1)\n",
    "    datas[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VARmodel:\n",
    "    def __init__(self, data, name='VARmodel', var_dim=None, mu=None, current_year=None):\n",
    "        self.num_samples = 100\n",
    "        self.data_raw = data\n",
    "        self.mu = mu\n",
    "        self.var_dim = var_dim\n",
    "        self.years = data.columns.values.astype('float32')\n",
    "        years_c = tf.constant(data.columns.values, dtype=tf.float32, name='data_years')\n",
    "\n",
    "        if current_year is None:\n",
    "            self.OBSERV_STEPS = np.Infinity\n",
    "        else:\n",
    "            self.OBSERV_STEPS = tf.reduce_sum(tf.cast(years_c <= current_year, tf.int32))\n",
    "\n",
    "        self.NUM_STEPS = data.shape[1]\n",
    "        self.name = name\n",
    "        self.logdensities = []\n",
    "        self.priors = []\n",
    "        self.dim = [self.var_dim,self.var_dim*2+1]\n",
    "        self.summaries = []\n",
    "\n",
    "        self.observable_mask = tf.range(0, self.NUM_STEPS, dtype=tf.int32) < self.OBSERV_STEPS\n",
    "\n",
    "        pd = np.std(data.values[:,1:] - data.values[:,:-1], axis=-1).astype(floatX)[:self.var_dim]\n",
    "\n",
    "        with tf.variable_scope(name, dtype=floatX) as scope:\n",
    "            self.data = tf.get_variable(initializer=data.values.T[np.newaxis].astype(floatX),\n",
    "                                    trainable=False, name='data')\n",
    "            self.scope = scope\n",
    "\n",
    "            self.create_rw_priors()\n",
    "            self.outputs = self.create_walk_inference(mu=mu)\n",
    "            self.create_observ_dispersion_inference(pd*0.5)\n",
    "            self.create_likelihood(self.observable_mask, self.outputs)\n",
    "\n",
    "    def create_summary(self, stype, name, tensor):\n",
    "        s = stype(name, tensor)\n",
    "        self.summaries.append(s)\n",
    "\n",
    "    def create_rw_priors(self):\n",
    "        dim = self.dim\n",
    "        with tf.variable_scope('rw_priors'):\n",
    "            s1 = 0.01/4\n",
    "            cov_prior = Normal(dim=None, mu=0.5*math.log(1/s1), sigma=3.5, name='cov_prior')\n",
    "\n",
    "            with tf.variable_scope('PWalk_inf'):\n",
    "                with tf.variable_scope('flows'):\n",
    "                    flow_conf = [NVPFlow(dim=self.dim[0]*self.dim[1], name='nvp_{}'.format(i)) for i in range(4)] + \\\n",
    "                        [LinearChol(dim=self.dim[0]*self.dim[1], name='lc')]\n",
    "                    ldiag = DFlow(flow_conf, num_samples=self.num_samples)\n",
    "                    print(ldiag.logdens)\n",
    "                    ldiag.output += 0.5*math.log(1/s1)\n",
    "                    ldiag.logdens -= tf.reduce_sum(ldiag.output, axis=-1)\n",
    "                    print('ldiag logdens', ldiag.logdens)\n",
    "\n",
    "                    self.logdensities.append(ldiag.logdens)\n",
    "                \n",
    "                if self.mu is None:\n",
    "                    sigma0 = None\n",
    "                else:\n",
    "                    sigma0 = 3.\n",
    "                    \n",
    "                PWalk = MVNormalRW(dim=self.dim[0]*self.dim[1], \n",
    "                                   sigma0=3., \n",
    "                                   diag=tf.exp(ldiag.output), name='OrdWalk')\n",
    "                self.priors.append(tf.reduce_sum(cov_prior.logdens(ldiag.output, reduce=False), axis=1))\n",
    "                self.PWalk = PWalk\n",
    "                tf.summary.scalar('s1_ord', tf.reduce_mean(tf.sqrt(PWalk.diag)))\n",
    "\n",
    "    def create_walk_inference(self, mu=None):\n",
    "        dim = self.dim\n",
    "        gvar = GVAR(dim=dim[0]*dim[1], len=self.NUM_STEPS, name='coef_rw_inference', mu=mu, num_samples=self.num_samples)\n",
    "        outputs = gvar.sample()\n",
    "\n",
    "        self.logdensities.append(gvar.logdens)\n",
    "        with tf.name_scope('PWald_prior'):\n",
    "            self.priors.append(tf.reduce_sum(self.PWalk.logdens(outputs), axis=[1]))\n",
    "        self.outputs = outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def create_observ_dispersion_inference(self, prior_disp):\n",
    "        print('Prior disp: {}'.format(prior_disp))\n",
    "        prior_loc = -0.5*np.log(prior_disp).astype(floatX)\n",
    "\n",
    "        with tf.variable_scope('obs_d_inf', reuse=tf.AUTO_REUSE):\n",
    "            flow_conf = [NVPFlow(dim=self.var_dim, name='nvp_{}'.format(i)) for i in range(6)] + \\\n",
    "                [LinearChol(dim=self.var_dim, name='lc')]\n",
    "            ldiag = DFlow(flow_conf, init_sigma=0.05, num_samples=self.num_samples)\n",
    "\n",
    "            ldiag.output -= prior_loc\n",
    "            ldiag.logdens -= tf.reduce_sum(ldiag.output, axis=-1)\n",
    "\n",
    "        self.obs_d = tf.distributions.Normal(tf.constant(0., dtype=floatX), \n",
    "                                             scale=tf.exp(ldiag.output), name='obs_d_prior')\n",
    "        \n",
    "        with tf.name_scope('obsrv_prior'):\n",
    "            pr = tf.reduce_sum(tf.distributions.Normal(loc=prior_loc[np.newaxis], \n",
    "                                                       scale=tf.constant(3., dtype=floatX)).log_prob(ldiag.output), \n",
    "                               axis=-1)\n",
    "        tf.summary.scalar('mean_ods', tf.reduce_mean(tf.exp(ldiag.output)))\n",
    "        self.logdensities.append(ldiag.logdens)\n",
    "        self.priors.append(pr)\n",
    "\n",
    "    def predict(self, observable_mask, outputs):\n",
    "        dim = self.dim\n",
    "        data = self.data\n",
    "        out = tf.reshape(outputs, [self.num_samples, self.NUM_STEPS, dim[0], dim[1]])\n",
    "        out = tf.transpose(out, [1,0,2,3])\n",
    "\n",
    "        def step(prev, x):\n",
    "            mask = x[0]\n",
    "            prev_pred = tf.where(mask, x[1], prev)\n",
    "            params = x[2]\n",
    "\n",
    "            d0 = params[:,:,:dim[0]]\n",
    "            d1 = params[:,:,dim[0]:2*dim[0]]\n",
    "\n",
    "            pp1 = prev_pred[:,:dim[0]]\n",
    "            pp0 = prev_pred[:,dim[0]:2*dim[0]]\n",
    "\n",
    "            new_pred = tf.einsum('bij,bj->bi', d0, pp0) + tf.einsum('bij,bj->bi', d1, pp1)+ params[:,:,-1] + pp1\n",
    "            obs_noise = self.obs_d.sample()\n",
    "            new_pred = tf.where(mask, new_pred, new_pred + obs_noise)\n",
    "\n",
    "            new_pred = tf.concat([new_pred, pp1], axis=1)\n",
    "            return new_pred\n",
    "        \n",
    "        data = tf.transpose(tf.tile(data, [self.num_samples, 1, 1]), [1,0,2])\n",
    "        ar = tf.scan(step, [observable_mask, data, out], \n",
    "                     initializer=tf.zeros([self.num_samples, 2*dim[0]], dtype=floatX))\n",
    "        return ar\n",
    "\n",
    "    def create_likelihood(self, observable_mask, outputs):\n",
    "        dim = self.dim\n",
    "        obs_d = self.obs_d\n",
    "\n",
    "        preds = self.predict(observable_mask, outputs)\n",
    "        self.preds = preds[:,:self.var_dim]\n",
    "        \n",
    "        with tf.name_scope('loglikelihood'):\n",
    "            data = tf.transpose(self.data, [1,0,2])\n",
    "            diffs = preds[:-1,:] - data[1:,:]\n",
    "            diffs = diffs[:,:,:dim[0]]\n",
    "\n",
    "            logl = tf.reduce_sum(obs_d.log_prob(diffs), [2])\n",
    "            print(logl)\n",
    "            logl *= tf.cast(self.observable_mask[1:], floatX)[:,tf.newaxis]\n",
    "\n",
    "            logl = tf.reduce_sum(logl, axis=0)\n",
    "            self.priors.append(logl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = {c:d for c,d in zip(ccodes, datas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 192, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datas[0][0]\n",
    "d = pd.DataFrame(d.T, columns=range(d.shape[0]))\n",
    "datas[0] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = '/cpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/SYNTH/rw_priors/PWalk_inf/flows/sub:0\", shape=(100,), dtype=float32)\n",
      "ldiag logdens Tensor(\"model/SYNTH/rw_priors/PWalk_inf/flows/sub_1:0\", shape=(100,), dtype=float32)\n",
      "Prior disp: [0.49601343 0.5011316  0.4979925  0.5013071 ]\n",
      "Tensor(\"model/SYNTH/loglikelihood/Sum:0\", shape=(191, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# with tf.device(device):\n",
    "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
    "    model = VARmodel(datas[0], name='SYNTH', var_dim=4, current_year=3000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'model/SYNTH/rw_priors/PWalk_inf/Sum:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'model/SYNTH/PWald_prior/Sum:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'model/SYNTH/obsrv_prior/Sum:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'model/SYNTH/loglikelihood/Sum_1:0' shape=(100,) dtype=float32>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'model/SYNTH/rw_priors/PWalk_inf/flows/sub_1:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'model/SYNTH/coef_rw_inference_1/VAR/logdens/add:0' shape=(100,) dtype=float32>,\n",
       " <tf.Tensor 'model/SYNTH/obs_d_inf/sub_2:0' shape=(100,) dtype=float32>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logdensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(device):\n",
    "prior = tf.add_n(model.priors)\n",
    "\n",
    "logdensity = tf.add_n(model.logdensities)\n",
    "kl = logdensity - prior\n",
    "kl = tf.reduce_mean(kl)\n",
    "kl /= 36*160\n",
    "\n",
    "main_op = tf.train.AdamOptimizer(0.001).minimize(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kls = tf.summary.scalar('KLd', kl)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('/home/nikita/tmp/tblogs/multisample100_float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, 100000):\n",
    "    print(epoch)\n",
    "    for step in range(100):\n",
    "        sess.run(main_op)\n",
    "    s, _ = sess.run([summary, main_op])\n",
    "    writer.add_summary(s, global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = []\n",
    "for _ in range(4000):\n",
    "    d = np.sqrt(np.diag(model.PWalk.sigma.eval()))\n",
    "    diags.append(d)\n",
    "diags = np.mean(diags, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsigmas = tf.sqrt(tf.diag_part(model.PWalk.sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_post=global_inf.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "for _ in range(1000):\n",
    "    ss.append(wsigmas.eval())\n",
    "ss = np.array(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ss,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ss[:,4], ss[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6_gpu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
