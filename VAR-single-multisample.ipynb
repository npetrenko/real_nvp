{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from flows import NormalRW, DFlow, NVPFlow, LogNormal, GVAR, phase,\\\n",
    "Normal, floatX, MVNormal, MVNormalRW, Linear, LinearChol\n",
    "import flows\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.contrib.distributions import WishartCholesky\n",
    "import math\n",
    "from flows.models import VARmodel\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUS.csv  FRA.csv  GBR.csv  SYNTH.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls CDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccodes = ['FRA']#, 'FRA', 'GBR']\n",
    "datas = ['./CDATA/{}.csv'.format(x) for x in ccodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [pd.read_csv(x, index_col='VARIABLE').values.astype(floatX).T[np.newaxis][:,1:-1] for x in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 193, 4)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data.shape for data in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00571698 0.93158025 0.00503056 0.03016526]\n",
      "---\n",
      "[0.99999994 0.99999994 0.99999934 1.        ]\n"
     ]
    }
   ],
   "source": [
    "scaler = 0.\n",
    "for i, data in enumerate(datas):\n",
    "    stds = (data[0,1:] - data[0,:-1]).std(axis=0)\n",
    "    print(stds)\n",
    "    scaler = scaler + stds\n",
    "    datas[i] = data\n",
    "print('---')\n",
    "scaler /= len(datas)\n",
    "for i in range(len(datas)):\n",
    "    datas[i] /= scaler\n",
    "    data = datas[i]\n",
    "    stds = (data[0,1:] - data[0,:-1]).std(axis=0)\n",
    "    print(stds)\n",
    "    data = np.concatenate([data[:,1:], data[:,:-1]], axis=-1)\n",
    "    datas[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VARmodel:\n",
    "    def __init__(self, data, name='VARmodel', var_dim=None, mu=None, current_year=None):\n",
    "        self.num_samples = 2000\n",
    "        self.data_raw = data\n",
    "        self.mu = mu\n",
    "        self.var_dim = var_dim\n",
    "        self.years = data.columns.values.astype('float32')\n",
    "        years_c = tf.constant(data.columns.values, dtype=tf.float32, name='data_years')\n",
    "\n",
    "        if current_year is None:\n",
    "            self.OBSERV_STEPS = np.Infinity\n",
    "        else:\n",
    "            self.OBSERV_STEPS = tf.reduce_sum(tf.cast(years_c <= current_year, tf.int32))\n",
    "\n",
    "        self.NUM_STEPS = data.shape[1]\n",
    "        self.name = name\n",
    "        self.logdensities = []\n",
    "        self.priors = []\n",
    "        self.dim = [self.var_dim,self.var_dim*2+1]\n",
    "        self.summaries = []\n",
    "\n",
    "        self.observable_mask = tf.range(0, self.NUM_STEPS, dtype=tf.int32) < self.OBSERV_STEPS\n",
    "\n",
    "        pd = np.std(data.values[:,1:] - data.values[:,:-1], axis=-1).astype(floatX)[:self.var_dim]\n",
    "\n",
    "        with tf.variable_scope(name, dtype=floatX) as scope:\n",
    "            self.data = tf.get_variable(initializer=data.values.T[np.newaxis].astype(floatX),\n",
    "                                    trainable=False, name='data')\n",
    "            self.scope = scope\n",
    "\n",
    "            self.create_rw_priors()\n",
    "            self.outputs = self.create_walk_inference(mu=mu)\n",
    "            self.create_observ_dispersion_inference(pd*0.5)\n",
    "            self.create_likelihood(self.observable_mask, self.outputs)\n",
    "\n",
    "    def create_summary(self, stype, name, tensor):\n",
    "        s = stype(name, tensor)\n",
    "        self.summaries.append(s)\n",
    "\n",
    "    def create_rw_priors(self):\n",
    "        dim = self.dim\n",
    "        with tf.variable_scope('rw_priors'):\n",
    "            s1 = 0.01/4\n",
    "            cov_prior = Normal(dim=None, mu=0.5*math.log(s1), sigma=3.5, name='cov_prior')\n",
    "\n",
    "            with tf.variable_scope('PWalk_inf'):\n",
    "                with tf.variable_scope('flows'):\n",
    "                    flow_conf = [NVPFlow(dim=self.dim[0]*self.dim[1], name='nvp_{}'.format(i)) for i in range(4)] + \\\n",
    "                        [LinearChol(dim=self.dim[0]*self.dim[1], name='lc')]\n",
    "                    ldiag = DFlow(flow_conf, num_samples=self.num_samples)\n",
    "                    ldiag.output += 0.5*math.log(s1)\n",
    "                    ldiag.logdens -= tf.reduce_sum(ldiag.output, axis=-1)\n",
    "                    print('ldiag logdens', ldiag.logdens)\n",
    "\n",
    "                    self.logdensities.append(ldiag.logdens)\n",
    "                \n",
    "                if self.mu is None:\n",
    "                    sigma0 = None\n",
    "                else:\n",
    "                    sigma0 = 3.\n",
    "                    \n",
    "                PWalk = MVNormalRW(dim=self.dim[0]*self.dim[1], \n",
    "                                   sigma0=3., \n",
    "                                   diag=tf.exp(ldiag.output), name='OrdWalk')\n",
    "                self.priors.append(tf.reduce_sum(cov_prior.logdens(ldiag.output, reduce=False), axis=1))\n",
    "                self.PWalk = PWalk\n",
    "                tf.summary.scalar('s1_ord', tf.reduce_mean(tf.sqrt(PWalk.diag)))\n",
    "\n",
    "    def create_walk_inference(self, mu=None):\n",
    "        dim = self.dim\n",
    "        gvar = GVAR(dim=dim[0]*dim[1], len=self.NUM_STEPS, name='coef_rw_inference', \n",
    "                    mu=mu, num_samples=self.num_samples)\n",
    "        outputs = gvar.sample()\n",
    "\n",
    "        self.logdensities.append(gvar.logdens)\n",
    "        with tf.name_scope('PWald_prior'):\n",
    "            pwld = self.PWalk.logdens(outputs)\n",
    "            print('pwld', pwld)\n",
    "            self.priors.append(tf.reduce_sum(pwld, axis=[1]))\n",
    "        self.outputs = outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def create_observ_dispersion_inference(self, prior_disp):\n",
    "        print('Prior disp: {}'.format(prior_disp))\n",
    "        prior_loc = -0.5*np.log(prior_disp).astype(floatX)\n",
    "\n",
    "        with tf.variable_scope('obs_d_inf', reuse=tf.AUTO_REUSE):\n",
    "            flow_conf = [NVPFlow(dim=self.var_dim, name='nvp_{}'.format(i)) for i in range(6)] + \\\n",
    "                [LinearChol(dim=self.var_dim, name='lc')]\n",
    "            ldiag = DFlow(flow_conf, init_sigma=0.05, num_samples=self.num_samples)\n",
    "\n",
    "            ldiag.output -= prior_loc\n",
    "            ldiag.logdens -= tf.reduce_sum(ldiag.output, axis=-1)\n",
    "\n",
    "        self.obs_d = tf.distributions.Normal(tf.constant(0., dtype=floatX), \n",
    "                                             scale=tf.exp(ldiag.output), name='obs_d_prior')\n",
    "        \n",
    "        with tf.name_scope('obsrv_prior'):\n",
    "            pr = tf.reduce_sum(tf.distributions.Normal(loc=prior_loc[np.newaxis], \n",
    "                                                       scale=tf.constant(3., dtype=floatX)).log_prob(ldiag.output), \n",
    "                               axis=-1)\n",
    "        tf.summary.scalar('mean_ods', tf.reduce_mean(tf.exp(ldiag.output)))\n",
    "        self.logdensities.append(ldiag.logdens)\n",
    "        self.priors.append(pr)\n",
    "\n",
    "    def predict(self, observable_mask, outputs):\n",
    "        dim = self.dim\n",
    "        data = self.data\n",
    "        out = tf.reshape(outputs, [self.num_samples, self.NUM_STEPS, dim[0], dim[1]])\n",
    "        out = tf.transpose(out, [1,0,2,3])\n",
    "\n",
    "        def step(prev, x):\n",
    "            mask = x[0]\n",
    "            prev_pred = tf.where(mask, x[1], prev)\n",
    "            params = x[2]\n",
    "\n",
    "            d0 = params[:,:,:dim[0]]\n",
    "            d1 = params[:,:,dim[0]:2*dim[0]]\n",
    "\n",
    "            pp1 = prev_pred[:,:dim[0]]\n",
    "            pp0 = prev_pred[:,dim[0]:2*dim[0]]\n",
    "\n",
    "            new_pred = tf.einsum('bij,bj->bi', d0, pp0) + tf.einsum('bij,bj->bi', d1, pp1)+ params[:,:,-1] + pp1\n",
    "            obs_noise = self.obs_d.sample()\n",
    "            new_pred = tf.where(mask, new_pred, new_pred + obs_noise)\n",
    "\n",
    "            new_pred = tf.concat([new_pred, pp1], axis=1)\n",
    "            return new_pred\n",
    "        \n",
    "        data = tf.transpose(tf.tile(data, [self.num_samples, 1, 1]), [1,0,2])\n",
    "        ar = tf.scan(step, [observable_mask, data, out], \n",
    "                     initializer=tf.zeros([self.num_samples, 2*dim[0]], dtype=floatX))\n",
    "        return ar\n",
    "\n",
    "    def create_likelihood(self, observable_mask, outputs):\n",
    "        dim = self.dim\n",
    "        obs_d = self.obs_d\n",
    "\n",
    "        preds = self.predict(observable_mask, outputs)\n",
    "        self.preds = preds[:,:self.var_dim]\n",
    "        \n",
    "        with tf.name_scope('loglikelihood'):\n",
    "            data = tf.transpose(self.data, [1,0,2])\n",
    "            diffs = preds[:-1,:] - data[1:,:]\n",
    "            diffs = diffs[:,:,:dim[0]]\n",
    "\n",
    "            logl = tf.reduce_sum(obs_d.log_prob(diffs), [2])\n",
    "            print('logl', logl)\n",
    "            logl *= tf.cast(self.observable_mask[1:], floatX)[:,tf.newaxis]\n",
    "\n",
    "            logl = tf.reduce_sum(logl, axis=0)\n",
    "            self.priors.append(logl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flows.models import VARmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = {c:d for c,d in zip(ccodes, datas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 192, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datas[0][0]\n",
    "d = pd.DataFrame(d.T, columns=range(d.shape[0]))\n",
    "datas[0] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldiag logdens Tensor(\"model/FRA/rw_priors/PWalk_inf/flows/sub_1:0\", shape=(1024,), dtype=float32)\n",
      "pwld Tensor(\"model/FRA/PWald_prior/OrdWalk/logdens/concat:0\", shape=(1024, 192), dtype=float32)\n",
      "Prior disp: [0.49601343 0.5011316  0.4979925  0.5013071 ]\n",
      "preds Tensor(\"model/FRA/strided_slice:0\", shape=(192, 1024, 4), dtype=float32)\n",
      "blogl Tensor(\"model/FRA/loglikelihood/mul:0\", shape=(191, 1024), dtype=float32)\n",
      "logl Tensor(\"model/FRA/loglikelihood/Sum_1:0\", shape=(1024,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
    "    model = VARmodel(datas[0], name='FRA', var_dim=4, current_year=3000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'model/FRA/rw_priors/PWalk_inf/Sum:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'model/FRA/PWald_prior/Sum:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'model/FRA/obsrv_prior/Sum:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'model/FRA/loglikelihood/Sum_1:0' shape=(1024,) dtype=float32>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'model/FRA/rw_priors/PWalk_inf/flows/sub_1:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'model/FRA/coef_rw_inference_1/VAR/logdens/add:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'model/FRA/obs_d_inf/sub_1:0' shape=(1024,) dtype=float32>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logdensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"AddN:0\", shape=(1024,), dtype=float32) Tensor(\"AddN_1:0\", shape=(1024,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "prior = tf.add_n(model.priors)\n",
    "logdensity = tf.add_n(model.logdensities)\n",
    "print(prior, logdensity)\n",
    "\n",
    "kl = logdensity - prior\n",
    "kl = tf.reduce_mean(kl)\n",
    "kl /= 36*160\n",
    "\n",
    "lr = tf.constant(0.001)\n",
    "main_op = tf.train.AdamOptimizer(lr).minimize(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kls = tf.summary.scalar('KLd', kl)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0691975 , 0.23756538, 0.06122474, 0.18523894, 0.06376822,\n",
       "       0.19053948, 0.07310431, 0.00921624, 0.31993693, 0.05851084,\n",
       "       0.15029831, 0.05845614, 0.11992872, 0.0468922 , 0.14463224,\n",
       "       0.1986526 , 0.01132096, 0.01153937, 0.08370733, 0.01362971,\n",
       "       0.01045661, 0.01223395, 0.12885961, 0.01372957, 0.11040618,\n",
       "       0.01482133, 0.01552367, 0.18504989, 0.01410815, 0.01324318,\n",
       "       0.01765098, 0.02276933, 0.01394978, 0.02784467, 0.02197746,\n",
       "       0.01521921], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.PWalk.diag.eval().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00403532, 0.00914537, 0.00357073, 0.00753403, 0.00386657,\n",
       "       0.00681846, 0.00473926, 0.00043602, 0.01328854, 0.00314246,\n",
       "       0.00769496, 0.0032874 , 0.00421772, 0.00260856, 0.00556762,\n",
       "       0.00874742, 0.00052711, 0.00052822, 0.00438387, 0.00057347,\n",
       "       0.00063281, 0.00052898, 0.00624812, 0.00057629, 0.00559987,\n",
       "       0.00063934, 0.0006772 , 0.00913313, 0.00061477, 0.00071913,\n",
       "       0.00076966, 0.00106164, 0.00061931, 0.00134845, 0.00091614,\n",
       "       0.0006671 ], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.PWalk.diag.eval().std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('/tmp/tfdbg/single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8a84091c95f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, 100000):\n",
    "    print(epoch)\n",
    "    for step in range(10):\n",
    "        sess.run(main_op)\n",
    "    s, _ = sess.run([summary, main_op], {lr: 0.001})\n",
    "    writer.add_summary(s, global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.preds.eval().mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = model.data_raw.values[:4,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45195356, 0.66985625, 0.6350035 , 0.67699057], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(preds - dd, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = []\n",
    "for _ in range(4000):\n",
    "    d = model.PWalk.diag.eval().mean(axis=0)\n",
    "    diags.append(d)\n",
    "diags = np.mean(diags, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00546182, 0.01633643, 0.01477936, 0.01048888, 0.01622361,\n",
       "       0.01212972, 0.01796927, 0.00991377, 0.01518895, 0.01184873,\n",
       "       0.0191168 , 0.01572438, 0.01451007, 0.01222456, 0.02210707,\n",
       "       0.02022636, 0.0124552 , 0.01169597, 0.01707046, 0.01198687,\n",
       "       0.01595444, 0.00960567, 0.03551232, 0.01234829, 0.03540852,\n",
       "       0.01410678, 0.01352153, 0.01937663, 0.01219932, 0.02050923,\n",
       "       0.01727278, 0.01346721, 0.01197882, 0.02661569, 0.02181612,\n",
       "       0.01188068], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsigmas = tf.sqrt(tf.diag_part(model.PWalk.sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_post=global_inf.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "for _ in range(1000):\n",
    "    ss.append(wsigmas.eval())\n",
    "ss = np.array(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ss,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ss[:,4], ss[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6_gpu",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
