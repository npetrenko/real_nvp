{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "# from transliterate import translit\n",
    "sess = tf.InteractiveSession()\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translit('Привет!', 'ru', reversed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('./compound.txt','r', errors='ignore') as f:\n",
    "    for l in f:\n",
    "        lines.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ' !\"()-0123456789:;.,?abcdefghijklmnopqrstuvwxyzабвгдежзийклмнопрстуфхцчшщъыьэюяё'\n",
    "valid = set(valid)\n",
    "lines = [[it for it in line.lower() if it in valid] for line in lines]\n",
    "lines = map(lambda x: ''.join(x), lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subber = lambda x: re.sub('\\s+', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = map(subber, lines)\n",
    "lines = filter(lambda x: len(x) > 10, lines)\n",
    "lines = map(lambda x: x.strip(' ').strip().lower(), lines)\n",
    "lines = map(lambda x: x.split('.'), lines)\n",
    "lines = list(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sum(lines, [])\n",
    "lines = map(lambda x: x.split(','), lines)\n",
    "lines = sum(lines, [])\n",
    "lines = map(lambda x: x.strip(' '), lines)\n",
    "# lines = map(lambda x: translit(x, 'ru', reversed=True), lines)\n",
    "lines = list(filter(lambda x: len(x) > 10, lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'давным-давно (так давно'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = pd.read_csv('./all.csv')['content']\n",
    "# names = names.apply(lambda x: x.split('\\r\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = sum(names, [])\n",
    "# names = list(map(lambda x: x.strip(' ').lower(), names))\n",
    "# names = list(filter(lambda x: len(x) > 0, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = np.loadtxt('/home/nikita/tmp/Practical_RL/week7_[recap]_rnn/names', dtype=str, delimiter=';').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'что не сможет прийти'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subber(names[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120032"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(names)+'_'))\n",
    "dic = {x:i for i,x in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistLSTM:\n",
    "    def __init__(self, dim, name='DistLSTM', reuse=None):\n",
    "        self.dim = dim\n",
    "        self.name = name\n",
    "        self.reuse = reuse\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=reuse):\n",
    "            cells = [tf.nn.rnn_cell.LSTMCell(128, \n",
    "                                                name='cell_{}'.format(i), \n",
    "                                                activation=tf.nn.tanh) for i in range(3)]\n",
    "            self.cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            self.post_cell = lambda x: self.dense(x, dim, name='d1')\n",
    "            self.init_dist = tf.get_variable('init_dist',[1,dim], trainable=True,\n",
    "                                             initializer=tf.random_normal_initializer(stddev=0.01, mean=0.2))\n",
    "        \n",
    "    def forward_string_lookup(self, strings, dic):\n",
    "        inp = strings\n",
    "        \n",
    "        forward_lookup = tf.py_func(lambda x: self._convert_to_ix(x, dic), [inp], tf.int64)\n",
    "        forward_lookup = tf.reshape(forward_lookup, (-1,))\n",
    "        forward_lookup = tf.nn.embedding_lookup(tf.diag(tf.ones(len(chars))), forward_lookup)\n",
    "        forward_lookup = tf.reshape(forward_lookup, [tf.shape(inp)[0], -1, len(chars)])\n",
    "        forward_lookup = tf.cast(forward_lookup, tf.float32)\n",
    "        return forward_lookup\n",
    "    \n",
    "    def dense(self, inp, dim, name='dense'):\n",
    "        with tf.variable_scope(name, initializer=tf.random_normal_initializer(stddev=0.01)):\n",
    "            W = tf.get_variable('W', [inp.shape[-1], dim])\n",
    "            b = tf.get_variable('b', [1, dim])\n",
    "            out = tf.matmul(inp, W) + b\n",
    "        return out\n",
    "    \n",
    "    def logdens(self, seq):\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            batch_size, s_len = tf.shape(seq)[0], tf.shape(seq)[1]\n",
    "\n",
    "            cell = self.cell\n",
    "\n",
    "            s_t = tf.transpose(seq, [1,0,2])\n",
    "            init_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "            init = (tf.zeros([batch_size, cell.state_size[0][0]]), init_state)\n",
    "            out,_ = tf.scan(lambda prev, x: cell(x, prev[1]), s_t, initializer=init)\n",
    "            out = tf.transpose(out, [1,0,2])\n",
    "            \n",
    "            out_dim = out.shape\n",
    "            \n",
    "            out = tf.reshape(out, [-1, out_dim[-1]])\n",
    "            out = self.post_cell(out)\n",
    "            out = tf.reshape(out, [batch_size, s_len, self.dim])\n",
    "            \n",
    "            preds = out[:,:-1]\n",
    "            target = seq[:,1:]\n",
    "                        \n",
    "            init_logits = tf.tile(self.init_dist, [batch_size,1])\n",
    "            init_nll = tf.nn.softmax_cross_entropy_with_logits_v2(labels=seq[:,0], logits=init_logits)\n",
    "            init_nll = init_nll[:,tf.newaxis]\n",
    "            \n",
    "            nll = tf.nn.softmax_cross_entropy_with_logits_v2(labels=target, logits=preds)\n",
    "            nll = tf.concat([init_nll, nll], axis=1)\n",
    "            return -nll\n",
    "        \n",
    "    def sample(self):\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            init_sample = tf.distributions.Multinomial(total_count=1., logits=self.init_dist).sample()\n",
    "            \n",
    "            cell = self.cell\n",
    "\n",
    "            init_state = cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "\n",
    "            init = (init_sample, init_state)\n",
    "            \n",
    "            def step(prev):\n",
    "                x = prev[0]\n",
    "                state = prev[1]\n",
    "                cell_step = cell(x, state)\n",
    "                post_step = self.post_cell(cell_step[0])\n",
    "                post_step = tf.distributions.Multinomial(total_count=1., logits=post_step).sample()\n",
    "                return post_step, cell_step[1]\n",
    "            \n",
    "            out,_ = tf.scan(lambda prev, _: step(prev), tf.range(40), initializer=init)\n",
    "            out = tf.transpose(out, [1,0,2])\n",
    "            out = tf.concat([init_sample[:,tf.newaxis,:], out], axis=1)\n",
    "            return out                \n",
    "            \n",
    "    def backward_string_lookup(self, encs, dic):\n",
    "        encs = tf.cast(encs, tf.bool)\n",
    "        strs = tf.py_func(lambda x: self._convert_from_enc(x, dic), [encs], tf.string)\n",
    "        return strs\n",
    "        \n",
    "    @staticmethod\n",
    "    def _convert_to_ix(names, dic):\n",
    "        if type(names[0]) != str:\n",
    "            names = list(map(lambda x: x.decode('utf-8'), names))\n",
    "        chars = []\n",
    "        max_len = max([len(x) for x in names])\n",
    "        filler = dic['_']\n",
    "        for name in names:\n",
    "            chars.append([])\n",
    "            for s in name:\n",
    "                chars[-1].append(dic[s])\n",
    "            chars[-1] += [filler]*(max_len-len(name))\n",
    "        return np.array(chars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_from_enc(encs, dic):\n",
    "        rev_dic = {i:x for x,i in dic.items()}\n",
    "        ret = []\n",
    "        for row in encs:\n",
    "            table = np.array([range(len(dic))]*len(row))\n",
    "            ixs = table[row]\n",
    "            chars = [rev_dic[ix] for ix in ixs]\n",
    "            string = ''.join(chars)\n",
    "            ret.append(string)\n",
    "            print(string)\n",
    "        return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = names[:118000]\n",
    "test = names[118000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.Variable(np.array(train), trainable=False, name='train_data')\n",
    "test_data = tf.Variable(np.array(test), trainable=False, name='test_data')\n",
    "\n",
    "shuffle_op = tf.assign(train_data, tf.random_shuffle(train_data))\n",
    "train_crop = tf.random_crop(train_data, [50])\n",
    "# test_data = tf.random_crop(train_data, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlstm = DistLSTM(len(chars), reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_lk = dlstm.forward_string_lookup(train_crop, dic)\n",
    "\n",
    "fwd_lk_test = dlstm.forward_string_lookup(test_data, dic)\n",
    "\n",
    "train_loss = -tf.reduce_mean(dlstm.logdens(fwd_lk))\n",
    "\n",
    "test_loss = -tf.reduce_mean(dlstm.logdens(fwd_lk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dlstm.sample()\n",
    "reconstr = dlstm.backward_string_lookup(sample, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /tmp/tfdbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/tfdbg’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "train_sum = tf.summary.scalar('train_loss', train_loss)\n",
    "tf.summary.scalar('test_loss', test_loss)\n",
    "summary = tf.summary.merge_all()\n",
    "!mkdir /tmp/tfdbg\n",
    "writer = tf.summary.FileWriter('/tmp/tfdbg/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(0.0004).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "но вечером набы увидала__________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['но вечером набы увидала__________________'], dtype='<U41')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = sample.eval()\n",
    "dlstm._convert_from_enc(ss.astype('bool'), dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7760212\n",
      "с чгоми хилибаоным_______________________\n",
      "0.45494968\n",
      "поднялись к тому_________________________\n",
      "0.431212\n",
      "что васово отыплавивший вызяу этого______\n",
      "0.47667927\n",
      "но и скледвульно_________________________\n",
      "0.7776845\n",
      "что я бы_________________________________\n",
      "0.46388224\n",
      "лилиан павложали закорка и соглашившема в\n",
      "0.6489019\n",
      "мы в ранецник и потребуться нам востом___\n",
      "0.25645992\n",
      "что казалось_____________________________\n",
      "0.57164174\n",
      "что протянул рядой_______________________\n",
      "0.56273395\n",
      "я ужаснул лицо то________________________\n",
      "0.43044356\n",
      "как только же при избавления_____________\n",
      "0.49030954\n",
      "и не помнял крахвирый обраховом сотрестит\n",
      "0.4945532\n",
      "кроме сказались и побед__________________\n",
      "0.40282422\n",
      "хоть после свяци и была для конца большое\n",
      "0.45522383\n",
      "увна осыделся____________________________\n",
      "0.36085746\n",
      "что его было я страх добрыцтя____________\n",
      "0.50631845\n",
      "и к стуктере был стратают________________\n",
      "0.50028116\n",
      "и еще высаковица этого произошло и очень \n",
      "0.59362847\n",
      "что близко ненавистью бигу_______________\n",
      "0.46531773\n",
      "и простящения за долгом с фаршаже коротки\n",
      "0.5540761\n",
      "не глядя на вчеро еще облазил с вашим стр\n",
      "0.5258837\n",
      "в поздуех все рассользовалась тихого каза\n",
      "0.3903414\n",
      "привел как в москву______________________\n",
      "0.44695905\n",
      "если бы то не должне быть едешь пожабуетс\n",
      "0.7205623\n",
      "ветри рассказать вечнет__________________\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for batch in range(400):\n",
    "        opt.run()\n",
    "        if batch % 20 == 0:\n",
    "            s = train_sum.eval()\n",
    "            writer.add_summary(s)\n",
    "\n",
    "    print(train_loss.eval())\n",
    "    s = summary.eval()\n",
    "    writer.add_summary(s)\n",
    "    shuffle_op.eval()\n",
    "    \n",
    "    ss = sample.eval()\n",
    "    dlstm._convert_from_enc(ss.astype('bool'), dic)\n",
    "#     print(reconstr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/save\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, '/tmp/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (sys p)",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
