{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "# from transliterate import translit\n",
    "sess = tf.InteractiveSession()\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('/home/nikita/tmp/compound.txt','r', errors='ignore') as f:\n",
    "    for l in f:\n",
    "        lines.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ' !\"()-0123456789:;.,?abcdefghijklmnopqrstuvwxyzабвгдежзийклмнопрстуфхцчшщъыьэюяё'\n",
    "valid = set(valid)\n",
    "lines = [[it for it in line.lower() if it in valid] for line in lines]\n",
    "lines = map(lambda x: ''.join(x), lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subber = lambda x: re.sub('\\s+', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = map(subber, lines)\n",
    "lines = filter(lambda x: len(x) > 10, lines)\n",
    "lines = map(lambda x: x.strip(' ').strip().lower(), lines)\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    line = re.split('\\.|,',line)\n",
    "    new_lines += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = map(lambda x: x.strip(' '), lines)\n",
    "lines = list(filter(lambda x: len(x) > 10, lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['джордж оруэлл',\n",
       " 'первая часть',\n",
       " 'был холодный ясный апрельский день',\n",
       " 'и часы пробили тринадцать',\n",
       " 'уткнув подбородок в грудь',\n",
       " 'чтобы спастись от злого ветра',\n",
       " 'уинстон смит торопливо шмыгнул за стеклянную дверь жилого дома победа',\n",
       " 'но все-таки впустил за собой вихрь зернистой пыли',\n",
       " 'в вестибюле пахло вареной капустой и старыми половиками',\n",
       " 'против входа на стене висел цветной плакат']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = pd.read_csv('./all.csv')['content']\n",
    "# names = names.apply(lambda x: x.split('\\r\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = sum(names, [])\n",
    "# names = list(map(lambda x: x.strip(' ').lower(), names))\n",
    "# names = list(filter(lambda x: len(x) > 0, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = np.loadtxt('/home/nikita/tmp/Practical_RL/week7_[recap]_rnn/names', dtype=str, delimiter=';').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'чтоб не встретиться с версиловым'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subber(names[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255541"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(names)+'_'))\n",
    "dic = {x:i for i,x in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistLSTM:\n",
    "    def __init__(self, dim, name='DistLSTM', reuse=None):\n",
    "        self.dim = dim\n",
    "        self.name = name\n",
    "        self.reuse = reuse\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=reuse):\n",
    "            cells = [tf.nn.rnn_cell.LSTMCell(512, \n",
    "                                                name='cell_{}'.format(i), \n",
    "                                                activation=tf.nn.tanh) for i in range(3)]\n",
    "            self.cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            self.post_cell = lambda x: self.dense(x, dim, name='d1')\n",
    "            self.init_dist = tf.get_variable('init_dist',[1,dim], trainable=True,\n",
    "                                             initializer=tf.random_normal_initializer(stddev=0.01, mean=0.2))\n",
    "        \n",
    "    def forward_string_lookup(self, strings, dic):\n",
    "        inp = strings\n",
    "        \n",
    "        forward_lookup = tf.py_func(lambda x: self._convert_to_ix(x, dic), [inp], tf.int64)\n",
    "        forward_lookup = tf.reshape(forward_lookup, (-1,))\n",
    "        forward_lookup = tf.nn.embedding_lookup(tf.diag(tf.ones(len(chars))), forward_lookup)\n",
    "        forward_lookup = tf.reshape(forward_lookup, [tf.shape(inp)[0], -1, len(chars)])\n",
    "        forward_lookup = tf.cast(forward_lookup, tf.float32)\n",
    "        return forward_lookup\n",
    "    \n",
    "    def dense(self, inp, dim, name='dense'):\n",
    "        with tf.variable_scope(name, initializer=tf.random_normal_initializer(stddev=0.01)):\n",
    "            W = tf.get_variable('W', [inp.shape[-1], dim])\n",
    "            b = tf.get_variable('b', [1, dim])\n",
    "            out = tf.matmul(inp, W) + b\n",
    "        return out\n",
    "    \n",
    "    def logdens(self, seq):\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            batch_size, s_len = tf.shape(seq)[0], tf.shape(seq)[1]\n",
    "\n",
    "            cell = self.cell\n",
    "\n",
    "            s_t = tf.transpose(seq, [1,0,2])\n",
    "            init_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "            init = (tf.zeros([batch_size, cell.state_size[0][0]]), init_state)\n",
    "            out,_ = tf.scan(lambda prev, x: cell(x, prev[1]), s_t, initializer=init)\n",
    "            out = tf.transpose(out, [1,0,2])\n",
    "            \n",
    "            out_dim = out.shape\n",
    "            \n",
    "            out = tf.reshape(out, [-1, out_dim[-1]])\n",
    "            out = self.post_cell(out)\n",
    "            out = tf.reshape(out, [batch_size, s_len, self.dim])\n",
    "            \n",
    "            preds = out[:,:-1]\n",
    "            target = seq[:,1:]\n",
    "                        \n",
    "            init_logits = tf.tile(self.init_dist, [batch_size,1])\n",
    "            init_nll = tf.nn.softmax_cross_entropy_with_logits_v2(labels=seq[:,0], logits=init_logits)\n",
    "            init_nll = init_nll[:,tf.newaxis]\n",
    "            \n",
    "            nll = tf.nn.softmax_cross_entropy_with_logits_v2(labels=target, logits=preds)\n",
    "            nll = tf.concat([init_nll, nll], axis=1)\n",
    "            return -nll\n",
    "        \n",
    "    def sample(self):\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            init_sample = tf.distributions.Multinomial(total_count=1., logits=self.init_dist).sample()\n",
    "            \n",
    "            cell = self.cell\n",
    "\n",
    "            init_state = cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "\n",
    "            init = (init_sample, init_state)\n",
    "            \n",
    "            def step(prev):\n",
    "                x = prev[0]\n",
    "                state = prev[1]\n",
    "                cell_step = cell(x, state)\n",
    "                post_step = self.post_cell(cell_step[0])\n",
    "                post_step = tf.distributions.Multinomial(total_count=1., logits=post_step).sample()\n",
    "                return post_step, cell_step[1]\n",
    "            \n",
    "            out,_ = tf.scan(lambda prev, _: step(prev), tf.range(40), initializer=init)\n",
    "            out = tf.transpose(out, [1,0,2])\n",
    "            out = tf.concat([init_sample[:,tf.newaxis,:], out], axis=1)\n",
    "            return out                \n",
    "            \n",
    "    def backward_string_lookup(self, encs, dic):\n",
    "        encs = tf.cast(encs, tf.bool)\n",
    "        strs = tf.py_func(lambda x: self._convert_from_enc(x, dic), [encs], tf.string)\n",
    "        return strs\n",
    "        \n",
    "    @staticmethod\n",
    "    def _convert_to_ix(names, dic):\n",
    "        if type(names[0]) != str:\n",
    "            names = list(map(lambda x: x.decode('utf-8'), names))\n",
    "        chars = []\n",
    "        max_len = max([len(x) for x in names])\n",
    "        filler = dic['_']\n",
    "        for name in names:\n",
    "            chars.append([])\n",
    "            for s in name:\n",
    "                chars[-1].append(dic[s])\n",
    "            chars[-1] += [filler]*(max_len-len(name))\n",
    "        return np.array(chars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_from_enc(encs, dic):\n",
    "        rev_dic = {i:x for x,i in dic.items()}\n",
    "        ret = []\n",
    "        for row in encs:\n",
    "            table = np.array([range(len(dic))]*len(row))\n",
    "            ixs = table[row]\n",
    "            chars = [rev_dic[ix] for ix in ixs]\n",
    "            string = ''.join(chars)\n",
    "            ret.append(string)\n",
    "            print(string)\n",
    "        return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255541"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = names[:230000]\n",
    "test = names[230000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(np.array(train))\n",
    "    test_data = tf.data.Dataset.from_tensor_slices(np.array(test))\n",
    "    test_data = test_data.shuffle(1000).repeat().batch(200)\n",
    "    test_sampler = test_data.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(np.array(train))\n",
    "    train_data = train_data.shuffle(1000).repeat().batch(200)\n",
    "    train_sampler = train_data.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_sample = train_sampler.get_next()\n",
    "    test_sample = test_sampler.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    dlstm = DistLSTM(len(chars), reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    fwd_lk = dlstm.forward_string_lookup(train_sample, dic)\n",
    "\n",
    "    fwd_lk_test = dlstm.forward_string_lookup(test_sample, dic)\n",
    "\n",
    "    train_loss = -tf.reduce_mean(dlstm.logdens(fwd_lk))\n",
    "\n",
    "    test_loss = -tf.reduce_mean(dlstm.logdens(fwd_lk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    sample = dlstm.sample()\n",
    "    reconstr = dlstm.backward_string_lookup(sample, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /tmp/tfdbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/tfdbg’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "train_sum = tf.summary.scalar('train_loss', train_loss)\n",
    "tf.summary.scalar('test_loss', test_loss)\n",
    "summary = tf.summary.merge_all()\n",
    "!mkdir /tmp/tfdbg\n",
    "writer = tf.summary.FileWriter('/tmp/tfdbg/cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    opt = tf.train.AdamOptimizer(0.0002).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ыц 0ul1h3г1бу\"lmхtt-4ххкfьяс6tq(ajcъiъqc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['4ыц 0ul1h3г1бу\"lmхtt-4ххкfьяс6tq(ajcъiъqc'], dtype='<U41')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = sample.eval()\n",
    "dlstm._convert_from_enc(ss.astype('bool'), dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    for batch in range(400):\n",
    "        opt.run()\n",
    "        if batch % 5 == 0:\n",
    "            s = train_sum.eval()\n",
    "            writer.add_summary(s, global_step=epoch*400 + batch)\n",
    "\n",
    "    s = summary.eval()\n",
    "    writer.add_summary(s)\n",
    "    \n",
    "    ss = sample.eval()\n",
    "    dlstm._convert_from_enc(ss.astype('bool'), dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/save\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, '/tmp/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (sys p)",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
