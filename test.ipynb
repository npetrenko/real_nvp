{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "# from transliterate import translit\n",
    "sess = tf.InteractiveSession()\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('./CDATA/AUS.csv', index_col='VARIABLE')\n",
    "d.columns = d.columns.astype('float')\n",
    "\n",
    "valid = [x for x in d.columns if x< 2017]\n",
    "d = d.loc[:, valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1969.5</th>\n",
       "      <th>1969.75</th>\n",
       "      <th>1970.0</th>\n",
       "      <th>1970.25</th>\n",
       "      <th>1970.5</th>\n",
       "      <th>1970.75</th>\n",
       "      <th>1971.0</th>\n",
       "      <th>1971.25</th>\n",
       "      <th>1971.5</th>\n",
       "      <th>1971.75</th>\n",
       "      <th>...</th>\n",
       "      <th>2014.5</th>\n",
       "      <th>2014.75</th>\n",
       "      <th>2015.0</th>\n",
       "      <th>2015.25</th>\n",
       "      <th>2015.5</th>\n",
       "      <th>2015.75</th>\n",
       "      <th>2016.0</th>\n",
       "      <th>2016.25</th>\n",
       "      <th>2016.5</th>\n",
       "      <th>2016.75</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VARIABLE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GDP</th>\n",
       "      <td>0.030219</td>\n",
       "      <td>0.035731</td>\n",
       "      <td>0.024817</td>\n",
       "      <td>0.025163</td>\n",
       "      <td>0.039406</td>\n",
       "      <td>0.007164</td>\n",
       "      <td>0.016588</td>\n",
       "      <td>0.036758</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.039698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002995</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.014605</td>\n",
       "      <td>0.008412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRS</th>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>2.383333</td>\n",
       "      <td>-1.950000</td>\n",
       "      <td>-0.483333</td>\n",
       "      <td>1.033333</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>-1.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>-0.043333</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>-0.286667</td>\n",
       "      <td>-0.270000</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>-0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PGDP</th>\n",
       "      <td>0.011497</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>0.011853</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009375</td>\n",
       "      <td>-0.003449</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.002307</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>-0.004764</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.012952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNR</th>\n",
       "      <td>-0.015307</td>\n",
       "      <td>-0.021791</td>\n",
       "      <td>0.147687</td>\n",
       "      <td>-0.206975</td>\n",
       "      <td>0.021211</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>0.086468</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.026061</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>-0.010940</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>-0.067394</td>\n",
       "      <td>-0.009638</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>-0.017305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1969.50   1969.75   1970.00   1970.25   1970.50   1970.75  \\\n",
       "VARIABLE                                                               \n",
       "GDP       0.030219  0.035731  0.024817  0.025163  0.039406  0.007164   \n",
       "IRS       0.483333  0.300000 -0.250000  0.766667  2.383333 -1.950000   \n",
       "PGDP      0.011497  0.020034  0.001766  0.003666  0.018495  0.008880   \n",
       "UNR      -0.015307 -0.021791  0.147687 -0.206975  0.021211 -0.001307   \n",
       "\n",
       "           1971.00   1971.25   1971.50   1971.75    ...      2014.50  \\\n",
       "VARIABLE                                            ...                \n",
       "GDP       0.016588  0.036758  0.022950  0.039698    ...    -0.002995   \n",
       "IRS      -0.483333  1.033333  0.516667 -1.266667    ...     0.053333   \n",
       "PGDP      0.011853  0.025612  0.018197  0.008502    ...    -0.009375   \n",
       "UNR       0.009848  0.033988  0.086468  0.015439    ...     0.002828   \n",
       "\n",
       "           2014.75   2015.00   2015.25   2015.50   2015.75   2016.00  \\\n",
       "VARIABLE                                                               \n",
       "GDP       0.000768  0.004592  0.009782 -0.000497  0.009481  0.001196   \n",
       "IRS      -0.043333  0.093333 -0.286667 -0.270000 -0.033333  0.086667   \n",
       "PGDP     -0.003449  0.000039 -0.000035 -0.002307  0.001163 -0.004764   \n",
       "UNR       0.026061  0.009997 -0.010940 -0.006265  0.018576 -0.067394   \n",
       "\n",
       "           2016.25   2016.50   2016.75  \n",
       "VARIABLE                                \n",
       "GDP       0.008731  0.014605  0.008412  \n",
       "IRS       0.060000 -0.210000 -0.280000  \n",
       "PGDP     -0.000496  0.006724  0.012952  \n",
       "UNR      -0.009638  0.004405 -0.017305  \n",
       "\n",
       "[4 rows x 190 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(scale=0.01, size=1000)\n",
    "b = a + np.random.normal(scale=0.0015, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.98848372],\n",
       "       [0.98848372, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.Variable(1.)\n",
    "r = tf.random_normal((), mean=0, stddev=mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmu = tf.gradients(r,[mu])[0]\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08691475"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmu.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import binom, factorial\n",
    "\n",
    "def f(i):\n",
    "    lam = 36./2.3\n",
    "    return np.exp(-lam)*(lam**i)/(factorial(i)*binom(36, i))\n",
    "\n",
    "dens = [f(i) for i in range(36)]\n",
    "plt.plot(dens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translit('Привет!', 'ru', reversed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('./compound.txt','r', errors='ignore') as f:\n",
    "    for l in f:\n",
    "        lines.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = ' !\"()-0123456789:;.,?abcdefghijklmnopqrstuvwxyzабвгдежзийклмнопрстуфхцчшщъыьэюяё'\n",
    "valid = set(valid)\n",
    "lines = [[it for it in line.lower() if it in valid] for line in lines]\n",
    "lines = map(lambda x: ''.join(x), lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subber = lambda x: re.sub('\\s+', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = map(subber, lines)\n",
    "lines = filter(lambda x: len(x) > 10, lines)\n",
    "lines = map(lambda x: x.strip(' ').strip().lower(), lines)\n",
    "lines = map(lambda x: x.split('.'), lines)\n",
    "lines = list(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sum(lines, [])\n",
    "lines = map(lambda x: x.split(','), lines)\n",
    "lines = sum(lines, [])\n",
    "lines = map(lambda x: x.strip(' '), lines)\n",
    "# lines = map(lambda x: translit(x, 'ru', reversed=True), lines)\n",
    "lines = list(filter(lambda x: len(x) > 10, lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'давным-давно (так давно'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = pd.read_csv('./all.csv')['content']\n",
    "# names = names.apply(lambda x: x.split('\\r\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = sum(names, [])\n",
    "# names = list(map(lambda x: x.strip(' ').lower(), names))\n",
    "# names = list(filter(lambda x: len(x) > 0, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = np.loadtxt('/home/nikita/tmp/Practical_RL/week7_[recap]_rnn/names', dtype=str, delimiter=';').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'что не сможет прийти'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subber(names[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120032"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(names)+'_'))\n",
    "dic = {x:i for i,x in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistLSTM:\n",
    "    def __init__(self, dim, name='DistLSTM', reuse=None):\n",
    "        self.dim = dim\n",
    "        self.name = name\n",
    "        self.reuse = reuse\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=reuse):\n",
    "            cells = [tf.nn.rnn_cell.LSTMCell(128, \n",
    "                                                name='cell_{}'.format(i), \n",
    "                                                activation=tf.nn.tanh) for i in range(3)]\n",
    "            self.cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            self.post_cell = lambda x: self.dense(x, dim, name='d1')\n",
    "            self.init_dist = tf.get_variable('init_dist',[1,dim], trainable=True,\n",
    "                                             initializer=tf.random_normal_initializer(stddev=0.01, mean=0.2))\n",
    "        \n",
    "    def forward_string_lookup(self, strings, dic):\n",
    "        inp = strings\n",
    "        \n",
    "        forward_lookup = tf.py_func(lambda x: self._convert_to_ix(x, dic), [inp], tf.int64)\n",
    "        forward_lookup = tf.reshape(forward_lookup, (-1,))\n",
    "        forward_lookup = tf.nn.embedding_lookup(tf.diag(tf.ones(len(chars))), forward_lookup)\n",
    "        forward_lookup = tf.reshape(forward_lookup, [tf.shape(inp)[0], -1, len(chars)])\n",
    "        forward_lookup = tf.cast(forward_lookup, tf.float32)\n",
    "        return forward_lookup\n",
    "    \n",
    "    def dense(self, inp, dim, name='dense'):\n",
    "        with tf.variable_scope(name, initializer=tf.random_normal_initializer(stddev=0.01)):\n",
    "            W = tf.get_variable('W', [inp.shape[-1], dim])\n",
    "            b = tf.get_variable('b', [1, dim])\n",
    "            out = tf.matmul(inp, W) + b\n",
    "        return out\n",
    "    \n",
    "    def logdens(self, seq):\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            batch_size, s_len = tf.shape(seq)[0], tf.shape(seq)[1]\n",
    "\n",
    "            cell = self.cell\n",
    "\n",
    "            s_t = tf.transpose(seq, [1,0,2])\n",
    "            init_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "            init = (tf.zeros([batch_size, cell.state_size[0][0]]), init_state)\n",
    "            out,_ = tf.scan(lambda prev, x: cell(x, prev[1]), s_t, initializer=init)\n",
    "            out = tf.transpose(out, [1,0,2])\n",
    "            \n",
    "            out_dim = out.shape\n",
    "            \n",
    "            out = tf.reshape(out, [-1, out_dim[-1]])\n",
    "            out = self.post_cell(out)\n",
    "            out = tf.reshape(out, [batch_size, s_len, self.dim])\n",
    "            \n",
    "            preds = out[:,:-1]\n",
    "            target = seq[:,1:]\n",
    "                        \n",
    "            init_logits = tf.tile(self.init_dist, [batch_size,1])\n",
    "            init_nll = tf.nn.softmax_cross_entropy_with_logits_v2(labels=seq[:,0], logits=init_logits)\n",
    "            init_nll = init_nll[:,tf.newaxis]\n",
    "            \n",
    "            nll = tf.nn.softmax_cross_entropy_with_logits_v2(labels=target, logits=preds)\n",
    "            nll = tf.concat([init_nll, nll], axis=1)\n",
    "            return -nll\n",
    "        \n",
    "    def sample(self):\n",
    "        with tf.variable_scope(self.name, reuse=self.reuse):\n",
    "            init_sample = tf.distributions.Multinomial(total_count=1., logits=self.init_dist).sample()\n",
    "            \n",
    "            cell = self.cell\n",
    "\n",
    "            init_state = cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "\n",
    "            init = (init_sample, init_state)\n",
    "            \n",
    "            def step(prev):\n",
    "                x = prev[0]\n",
    "                state = prev[1]\n",
    "                cell_step = cell(x, state)\n",
    "                post_step = self.post_cell(cell_step[0])\n",
    "                post_step = tf.distributions.Multinomial(total_count=1., logits=post_step).sample()\n",
    "                return post_step, cell_step[1]\n",
    "            \n",
    "            out,_ = tf.scan(lambda prev, _: step(prev), tf.range(40), initializer=init)\n",
    "            out = tf.transpose(out, [1,0,2])\n",
    "            out = tf.concat([init_sample[:,tf.newaxis,:], out], axis=1)\n",
    "            return out                \n",
    "            \n",
    "    def backward_string_lookup(self, encs, dic):\n",
    "        encs = tf.cast(encs, tf.bool)\n",
    "        strs = tf.py_func(lambda x: self._convert_from_enc(x, dic), [encs], tf.string)\n",
    "        return strs\n",
    "        \n",
    "    @staticmethod\n",
    "    def _convert_to_ix(names, dic):\n",
    "        if type(names[0]) != str:\n",
    "            names = list(map(lambda x: x.decode('utf-8'), names))\n",
    "        chars = []\n",
    "        max_len = max([len(x) for x in names])\n",
    "        filler = dic['_']\n",
    "        for name in names:\n",
    "            chars.append([])\n",
    "            for s in name:\n",
    "                chars[-1].append(dic[s])\n",
    "            chars[-1] += [filler]*(max_len-len(name))\n",
    "        return np.array(chars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _convert_from_enc(encs, dic):\n",
    "        rev_dic = {i:x for x,i in dic.items()}\n",
    "        ret = []\n",
    "        for row in encs:\n",
    "            table = np.array([range(len(dic))]*len(row))\n",
    "            ixs = table[row]\n",
    "            chars = [rev_dic[ix] for ix in ixs]\n",
    "            string = ''.join(chars)\n",
    "            ret.append(string)\n",
    "            print(string)\n",
    "        return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = names[:118000]\n",
    "test = names[118000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.Variable(np.array(train), trainable=False, name='train_data')\n",
    "test_data = tf.Variable(np.array(test), trainable=False, name='test_data')\n",
    "\n",
    "shuffle_op = tf.assign(train_data, tf.random_shuffle(train_data))\n",
    "train_crop = tf.random_crop(train_data, [50])\n",
    "# test_data = tf.random_crop(train_data, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlstm = DistLSTM(len(chars), reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_lk = dlstm.forward_string_lookup(train_crop, dic)\n",
    "\n",
    "fwd_lk_test = dlstm.forward_string_lookup(test_data, dic)\n",
    "\n",
    "train_loss = -tf.reduce_mean(dlstm.logdens(fwd_lk))\n",
    "\n",
    "test_loss = -tf.reduce_mean(dlstm.logdens(fwd_lk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dlstm.sample()\n",
    "reconstr = dlstm.backward_string_lookup(sample, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /tmp/tfdbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/tfdbg’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "train_sum = tf.summary.scalar('train_loss', train_loss)\n",
    "tf.summary.scalar('test_loss', test_loss)\n",
    "summary = tf.summary.merge_all()\n",
    "!mkdir /tmp/tfdbg\n",
    "writer = tf.summary.FileWriter('/tmp/tfdbg/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(0.0004).minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "но вечером набы увидала__________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['но вечером набы увидала__________________'], dtype='<U41')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = sample.eval()\n",
    "dlstm._convert_from_enc(ss.astype('bool'), dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7760212\n",
      "с чгоми хилибаоным_______________________\n",
      "0.45494968\n",
      "поднялись к тому_________________________\n",
      "0.431212\n",
      "что васово отыплавивший вызяу этого______\n",
      "0.47667927\n",
      "но и скледвульно_________________________\n",
      "0.7776845\n",
      "что я бы_________________________________\n",
      "0.46388224\n",
      "лилиан павложали закорка и соглашившема в\n",
      "0.6489019\n",
      "мы в ранецник и потребуться нам востом___\n",
      "0.25645992\n",
      "что казалось_____________________________\n",
      "0.57164174\n",
      "что протянул рядой_______________________\n",
      "0.56273395\n",
      "я ужаснул лицо то________________________\n",
      "0.43044356\n",
      "как только же при избавления_____________\n",
      "0.49030954\n",
      "и не помнял крахвирый обраховом сотрестит\n",
      "0.4945532\n",
      "кроме сказались и побед__________________\n",
      "0.40282422\n",
      "хоть после свяци и была для конца большое\n",
      "0.45522383\n",
      "увна осыделся____________________________\n",
      "0.36085746\n",
      "что его было я страх добрыцтя____________\n",
      "0.50631845\n",
      "и к стуктере был стратают________________\n",
      "0.50028116\n",
      "и еще высаковица этого произошло и очень \n",
      "0.59362847\n",
      "что близко ненавистью бигу_______________\n",
      "0.46531773\n",
      "и простящения за долгом с фаршаже коротки\n",
      "0.5540761\n",
      "не глядя на вчеро еще облазил с вашим стр\n",
      "0.5258837\n",
      "в поздуех все рассользовалась тихого каза\n",
      "0.3903414\n",
      "привел как в москву______________________\n",
      "0.44695905\n",
      "если бы то не должне быть едешь пожабуетс\n",
      "0.7205623\n",
      "ветри рассказать вечнет__________________\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for batch in range(400):\n",
    "        opt.run()\n",
    "        if batch % 20 == 0:\n",
    "            s = train_sum.eval()\n",
    "            writer.add_summary(s)\n",
    "\n",
    "    print(train_loss.eval())\n",
    "    s = summary.eval()\n",
    "    writer.add_summary(s)\n",
    "    shuffle_op.eval()\n",
    "    \n",
    "    ss = sample.eval()\n",
    "    dlstm._convert_from_enc(ss.astype('bool'), dic)\n",
    "#     print(reconstr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/save\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, '/tmp/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (sys p)",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
