{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from flows import NormalRW, DFlow, NVPFlow, LogNormal, GVAR, phase,\\\n",
    "Normal, floatX, MVNormal, MVNormalRW, Linear, LinearChol\n",
    "import flows\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.contrib.distributions import WishartCholesky\n",
    "import math\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUS.csv  FIN.csv  FRA.csv  GBR.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls CDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccodes = ['AUS', 'FRA', 'GBR']\n",
    "datas = ['./CDATA/{}.csv'.format(x) for x in ccodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./CDATA/AUS.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./CDATA/AUS.csv', './CDATA/FRA.csv', './CDATA/GBR.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./CDATA/AUS.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1979</th>\n",
       "      <th>1980</th>\n",
       "      <th>1981</th>\n",
       "      <th>1982</th>\n",
       "      <th>1983</th>\n",
       "      <th>1984</th>\n",
       "      <th>1985</th>\n",
       "      <th>1986</th>\n",
       "      <th>1987</th>\n",
       "      <th>1988</th>\n",
       "      <th>...</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90399</th>\n",
       "      <td>9.090001</td>\n",
       "      <td>10.126582</td>\n",
       "      <td>9.691745</td>\n",
       "      <td>11.145511</td>\n",
       "      <td>10.113563</td>\n",
       "      <td>3.950185</td>\n",
       "      <td>6.739049</td>\n",
       "      <td>9.084532</td>\n",
       "      <td>8.488746</td>\n",
       "      <td>7.231772</td>\n",
       "      <td>...</td>\n",
       "      <td>3.538487</td>\n",
       "      <td>2.332362</td>\n",
       "      <td>4.352643</td>\n",
       "      <td>1.820112</td>\n",
       "      <td>2.845226</td>\n",
       "      <td>3.303850</td>\n",
       "      <td>1.762780</td>\n",
       "      <td>2.449889</td>\n",
       "      <td>2.487923</td>\n",
       "      <td>1.508367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91023</th>\n",
       "      <td>0.455171</td>\n",
       "      <td>-0.030040</td>\n",
       "      <td>2.108038</td>\n",
       "      <td>1.404348</td>\n",
       "      <td>2.052476</td>\n",
       "      <td>3.365892</td>\n",
       "      <td>7.449047</td>\n",
       "      <td>8.087802</td>\n",
       "      <td>7.492809</td>\n",
       "      <td>6.410574</td>\n",
       "      <td>...</td>\n",
       "      <td>2.425093</td>\n",
       "      <td>3.067561</td>\n",
       "      <td>4.182300</td>\n",
       "      <td>1.044141</td>\n",
       "      <td>6.206931</td>\n",
       "      <td>1.462691</td>\n",
       "      <td>4.820785</td>\n",
       "      <td>6.356251</td>\n",
       "      <td>4.447267</td>\n",
       "      <td>6.321014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91252</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>6.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1979       1980      1981       1982       1983      1984  \\\n",
       "90399  9.090001  10.126582  9.691745  11.145511  10.113563  3.950185   \n",
       "91023  0.455171  -0.030040  2.108038   1.404348   2.052476  3.365892   \n",
       "91252  6.300000   6.100000  5.800000   7.200000  10.000000  9.000000   \n",
       "\n",
       "           1985      1986      1987      1988    ...         2006      2007  \\\n",
       "90399  6.739049  9.084532  8.488746  7.231772    ...     3.538487  2.332362   \n",
       "91023  7.449047  8.087802  7.492809  6.410574    ...     2.425093  3.067561   \n",
       "91252  8.300000  8.100000  8.100000  7.200000    ...     4.800000  4.400000   \n",
       "\n",
       "           2008      2009      2010      2011      2012      2013      2014  \\\n",
       "90399  4.352643  1.820112  2.845226  3.303850  1.762780  2.449889  2.487923   \n",
       "91023  4.182300  1.044141  6.206931  1.462691  4.820785  6.356251  4.447267   \n",
       "91252  4.200000  5.600000  5.200000  5.100000  5.200000  5.700000  6.100000   \n",
       "\n",
       "           2015  \n",
       "90399  1.508367  \n",
       "91023  6.321014  \n",
       "91252  6.100000  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990,\n",
      "            1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,\n",
      "            2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
      "            2013, 2014],\n",
      "           dtype='int64')\n",
      "Int64Index([1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981,\n",
      "            1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n",
      "            1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002],\n",
      "           dtype='int64')\n",
      "Int64Index([1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000,\n",
      "            2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,\n",
      "            2012],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "datas = [pd.read_csv(x, index_col='Unnamed: 0').iloc[:,:-1] for x in datas]\n",
    "for i, data in enumerate(datas):\n",
    "    data.columns = data.columns.astype('int')\n",
    "    data = data.astype(floatX)\n",
    "    \n",
    "    new_data = np.concatenate([data.values.T[1:], data.values.T[:-1]], axis=1)\n",
    "    new_data_columns = data.columns[1:]\n",
    "    new_data = pd.DataFrame(new_data.T, columns=new_data_columns)\n",
    "    data = new_data\n",
    "    datas[i] = data\n",
    "    print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 35), (6, 32), (6, 23)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data.shape for data in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 35)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = {c:d for c,d in zip(ccodes, datas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VARmodel:\n",
    "    def __init__(self, data, name='VARmodel', mu=None, current_year=None):\n",
    "        self.data_raw = data\n",
    "        self.years = data.columns.values.astype('int32')\n",
    "        years_c = tf.constant(data.columns.values, dtype=tf.int32, name='data_years')\n",
    "\n",
    "        if current_year is None:\n",
    "            self.OBSERV_STEPS = np.Infinity\n",
    "        else:\n",
    "            self.OBSERV_STEPS = tf.reduce_sum(tf.cast(years_c <= current_year, tf.int32))\n",
    "        \n",
    "        self.NUM_STEPS = data.shape[1]\n",
    "        self.name = name\n",
    "        self.logdensities = []\n",
    "        self.priors = []\n",
    "        self.dim = [3,3*2+1]\n",
    "        self.summaries = []\n",
    "        \n",
    "        self.observable_mask = tf.range(0, self.NUM_STEPS, dtype=tf.int32) < self.OBSERV_STEPS\n",
    "        \n",
    "        pd = np.mean(np.std(data.values[:,1:] - data.values[:,:-1], axis=-1))\n",
    "        \n",
    "        with tf.variable_scope(name) as scope:\n",
    "            self.data = tf.get_variable(initializer=data.values.T[np.newaxis], \n",
    "                                    trainable=False, name='data')\n",
    "            self.scope = scope\n",
    "            \n",
    "            self.create_rw_priors()\n",
    "            self.outputs = self.create_walk_inference(mu=mu)\n",
    "            self.create_observ_dispersion_inference(pd*0.5)\n",
    "            self.create_likelihood(self.observable_mask, self.outputs)\n",
    "            self.summary = tf.summary.merge(self.summaries)\n",
    "            \n",
    "    def create_summary(self, stype, name, tensor):\n",
    "        s = stype(name, tensor)\n",
    "        self.summaries.append(s)\n",
    "\n",
    "    def create_rw_priors(self):\n",
    "        dim = self.dim\n",
    "        with tf.variable_scope('rw_priors'):\n",
    "            with tf.variable_scope('walk_ord'):\n",
    "                s1_prior_d = LogNormal(1, mu=math.log(0.01), sigma=6., name='s1_prior')\n",
    "\n",
    "                with tf.variable_scope('s1_inference', dtype=floatX):\n",
    "                    mu = tf.get_variable('mu', shape=[1], \n",
    "                                         initializer=tf.constant_initializer(s1_prior_d.mu))\n",
    "\n",
    "                    logsigma_init = tf.constant_initializer(min(math.log(s1_prior_d.sigma), -1.))\n",
    "                    logsigma = tf.get_variable('logsigma', shape=[1], \n",
    "                                               initializer=logsigma_init)\n",
    "                    sigma = tf.exp(logsigma)\n",
    "                    s1_d = LogNormal(1, mu=mu, sigma=sigma)\n",
    "\n",
    "                s1 = s1_d.sample()\n",
    "                \n",
    "                self.create_summary(tf.summary.scalar, 's1_ord', s1[0])\n",
    "                \n",
    "                self.logdensities.append(s1_d.logdens(s1))\n",
    "\n",
    "                s1_prior = s1_prior_d.logdens(s1)\n",
    "                self.priors.append(s1_prior)\n",
    "                \n",
    "            PWalk = NormalRW(dim=None, sigma0=None, mu0=None, sigma=s1, name='OrdWalk')\n",
    "            self.PWalk = PWalk\n",
    "                \n",
    "    def create_walk_inference(self, mu=None):\n",
    "        dim = self.dim\n",
    "        gvar = GVAR(dim=dim[0]*dim[1], len=self.NUM_STEPS, name='coef_rw_inference', mu=mu)\n",
    "        outputs = gvar.sample()\n",
    "        \n",
    "        self.logdensities.append(gvar.logdens)\n",
    "        self.priors.append(self.PWalk.logdens(outputs, reduce=True))\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def create_observ_dispersion_inference(self, prior_disp):\n",
    "        with tf.variable_scope('obs_d_inf', reuse=tf.AUTO_REUSE):\n",
    "#             ldiag = DFlow([NVPFlow(dim=3, name='ldiag_flow_' + str(i)) for i in range(2)], init_sigma=0.05)\n",
    "            ldiag = DFlow([LinearChol(dim=3, name='ldiag_flow_' + str(i)) for i in range(1)], init_sigma=0.05)\n",
    "\n",
    "            ldiag.output -= 0.5*math.log(prior_disp)\n",
    "            ldiag.logdens -= tf.reduce_sum(ldiag.output, axis=-1)\n",
    "        \n",
    "        self.obs_d = MVNormal(3, sigma=None, name='obs_d_prior', \n",
    "                   ldiag=ldiag.output[0])\n",
    "        \n",
    "        df = 3\n",
    "        pmat = np.diag([(2./prior_disp)]*3)/df\n",
    "        cov_prior = WishartCholesky(df, pmat, cholesky_input_output_matrices=True)\n",
    "        \n",
    "        pr = cov_prior.log_prob(self.obs_d.fsigma)\n",
    "        self.logdensities.append(ldiag.logdens[0])\n",
    "        self.priors.append(pr)\n",
    "        \n",
    "        sigmas = tf.sqrt(tf.diag_part(self.obs_d.sigma))\n",
    "        \n",
    "        std = tf.nn.moments(self.data[0,1:,:3] - self.data[0,:-1,:3], axes=[0])[1]\n",
    "        print(std, sigmas)\n",
    "        rsquareds = 1 - sigmas/std\n",
    "        self.create_summary(tf.summary.scalar, 'rsquared_post_mean', tf.reduce_mean(rsquareds))\n",
    "        self.create_summary(tf.summary.histogram, 'post_rsquared', rsquareds)\n",
    "        self.create_summary(tf.summary.histogram, 'post_disp', sigmas)\n",
    "            \n",
    "    def predict(self, observable_mask, outputs):\n",
    "        dim = self.dim\n",
    "        data = self.data\n",
    "        out = tf.reshape(outputs, [self.NUM_STEPS, dim[0], dim[1]])\n",
    "\n",
    "        def step(prev, x):\n",
    "            mask = x[0]\n",
    "            prev_pred = tf.where(mask, x[1], prev)[tf.newaxis]\n",
    "            params = x[2]\n",
    "\n",
    "            d0 = params[:,:dim[0]]\n",
    "            d1 = params[:,dim[0]:2*dim[0]]\n",
    "\n",
    "            pp1 = prev_pred[:,:dim[0]]\n",
    "            pp0 = prev_pred[:,dim[0]:2*dim[0]]\n",
    "\n",
    "            new_pred = tf.matmul(pp0, d0)[0] + tf.matmul(pp1, d1)[0]+ params[:,-1] + pp1[0]\n",
    "            obs_noise = 0.#elf.obs_d.sample()\n",
    "            new_pred = tf.where(mask, new_pred, new_pred + obs_noise)\n",
    "            \n",
    "            new_pred = tf.concat([new_pred, pp1[0]], axis=0)\n",
    "            return new_pred\n",
    "\n",
    "        ar = tf.scan(step, [observable_mask, data[0], out], initializer=tf.zeros([2*dim[0]], dtype=floatX))\n",
    "        return ar\n",
    "    \n",
    "    def create_likelihood(self, observable_mask, outputs):\n",
    "        dim = self.dim\n",
    "        obs_d = self.obs_d\n",
    "        \n",
    "        preds = self.predict(observable_mask, outputs)\n",
    "        self.preds = preds[:,:3]\n",
    "        \n",
    "        diffs = preds[:-1] - self.data[0,1:]\n",
    "        diffs = diffs[:,:dim[0]]\n",
    "        \n",
    "        std = tf.nn.moments(self.data[0,1:,:3] - self.data[0,:-1,:3], axes=[0])[1]\n",
    "        rsq_obs = tf.reduce_mean(tf.square(diffs), axis=0)/std\n",
    "        rsq_obs = 1-tf.reduce_mean(rsq_obs)\n",
    "        self.create_summary(tf.summary.scalar, 'rsquared_observed', rsq_obs)\n",
    "\n",
    "        logl = obs_d.logdens(diffs, reduce=False)\n",
    "        logl *= tf.cast(self.observable_mask[1:], floatX)\n",
    "        \n",
    "        logl = tf.reduce_sum(logl)\n",
    "        self.create_summary(tf.summary.scalar, 'loglikelihood', logl)\n",
    "        self.priors.append(logl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = tf.placeholder(tf.int32, shape=(), name='current_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_inf = DFlow([NVPFlow(dim=(3*2+1)*3, name='flow_{}'.format(i)) for i in range(6)], init_sigma=0.08)\n",
    "# global_inf = DFlow([LinearChol(dim=(3*2+1)*3, name='flow_{}'.format(i)) for i in range(1)], init_sigma=0.08)\n",
    "\n",
    "global_prior = Normal(None, sigma=30.).logdens(global_inf.output)\n",
    "tf.add_to_collection('priors', global_prior)\n",
    "tf.add_to_collection('logdensities', global_inf.logdens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sub:0' shape=(1,) dtype=float64>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_inf.logdens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flow_5/add_3:0' shape=(1, 21) dtype=float64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_inf.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('variation_rate', dtype=floatX):\n",
    "#     with tf.variable_scope('prior'):\n",
    "#         vprior = tf.distributions.Exponential(rate=1/0.7)\n",
    "        \n",
    "    variation_prerate = tf.get_variable('prerate', initializer=math.log(0.7))\n",
    "    variation_rate = tf.exp(variation_prerate)\n",
    "#     variation_d = tf.distributions.Exponential(variation_rate)\n",
    "    variation = variation_rate#variation_d.sample()\n",
    "    \n",
    "#     lpd = tf.cast(variation_d.log_prob(variation), floatX)[tf.newaxis]\n",
    "#     lpp = tf.cast(vprior.log_prob(variation), floatX)\n",
    "    variation = tf.cast(variation, floatX)\n",
    "    \n",
    "    tf.summary.scalar('variation', variation)\n",
    "#     tf.add_to_collection('logdensities', lpd)\n",
    "#     tf.add_to_collection('priors', lpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'variation_rate/Cast:0' shape=() dtype=float64>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "individ_variation_prior = Normal((3*2+1)*3, sigma=variation, mu=global_inf.output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"AUS_model/coef_rw_inference_1/VAR/add:0\", shape=(35, 21), dtype=float64)\n",
      "Tensor(\"AUS_model/moments/Squeeze_1:0\", shape=(3,), dtype=float64) Tensor(\"AUS_model/Sqrt:0\", shape=(3,), dtype=float64)\n",
      "Tensor(\"FRA_model/coef_rw_inference_1/VAR/add:0\", shape=(32, 21), dtype=float64)\n",
      "Tensor(\"FRA_model/moments/Squeeze_1:0\", shape=(3,), dtype=float64) Tensor(\"FRA_model/Sqrt:0\", shape=(3,), dtype=float64)\n",
      "Tensor(\"GBR_model/coef_rw_inference_1/VAR/add:0\", shape=(23, 21), dtype=float64)\n",
      "Tensor(\"GBR_model/moments/Squeeze_1:0\", shape=(3,), dtype=float64) Tensor(\"GBR_model/Sqrt:0\", shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "indiv_logdens = []\n",
    "indiv_priors = []\n",
    "indivs = {}\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), dtype=floatX, reuse=tf.AUTO_REUSE):\n",
    "    for country, data in country_data.items():\n",
    "        with tf.variable_scope(country):\n",
    "#             individ_variation = DFlow([LinearChol((3*2+1)*3, name='lc')], init_sigma=0.08)\n",
    "            individ_variation = DFlow([NVPFlow((3*2+1)*3, \n",
    "                                               name='nvp_{}'.format(i), \n",
    "                                               aux_vars=global_inf.output) for i in range(6)], init_sigma=0.08)\n",
    "\n",
    "            ind = individ_variation.output[0]\n",
    "            indivs[country] = ind\n",
    "\n",
    "            indiv_logdens.append(individ_variation.logdens)\n",
    "            indiv_priors.append(individ_variation_prior.logdens(ind))\n",
    "\n",
    "        model = VARmodel(data, name='{}_model'.format(country), mu=ind[tf.newaxis], current_year=current_year)\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'AUS/sub:0' shape=(1,) dtype=float64>,\n",
       " <tf.Tensor 'FRA/sub:0' shape=(1,) dtype=float64>,\n",
       " <tf.Tensor 'GBR/sub:0' shape=(1,) dtype=float64>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indiv_logdens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "prior = tf.reduce_sum([model.priors for model in models])\\\n",
    "+ tf.reduce_sum(indiv_priors) + tf.reduce_sum(graph.get_collection('priors'))\n",
    "\n",
    "logdensity = tf.reduce_sum([model.logdensities for model in models])\\\n",
    "+ tf.reduce_sum(indiv_logdens) + tf.reduce_sum(graph.get_collection('logdensities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = logdensity - prior\n",
    "kl /= 36*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truediv:0' shape=() dtype=float64>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kls = tf.summary.scalar('KLd', kl)\n",
    "summary = tf.summary.merge_all()#tf.summary.merge([kls, tf.summary.scalar('prior', prior)] + [model.summaries for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_op = tf.train.AdamOptimizer(0.0001).minimize(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -R /home/nikita/tmp/hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('/home/nikita/tmp/hier/4main++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'AUS_model/strided_slice_5:0' shape=(35, 3) dtype=float64>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_year(year):\n",
    "    cdic = {model.name:model for model in models}\n",
    "    preds = {model.name:[] for model in models}\n",
    "    preds_t = {model.name: model.preds for model in models}\n",
    "    for step in range(1000):\n",
    "        preds_i = sess.run(preds_t, {current_year:year})\n",
    "        for k in preds.keys():\n",
    "            preds[k].append(preds_i[k][cdic[k].years > year])\n",
    "            \n",
    "    mean_pred = {k:np.mean(v, axis=0) for k,v in preds.items()}\n",
    "    current_vals = {model.name:model.data_raw.loc[:,year].values[:3]}\n",
    "    return mean_pred, current_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'AUS_model': array([[  6.9902475 ,  10.47738941,   7.21347428],\n",
       "         [  6.93888002,  11.08745949,   7.52077839],\n",
       "         [  6.96557878,  11.83085523,   7.96083573],\n",
       "         [  7.07702135,  12.75291006,   8.55601154],\n",
       "         [  7.262492  ,  13.88035811,   9.33242984],\n",
       "         [  7.5331935 ,  15.23308218,  10.3364206 ],\n",
       "         [  7.91036133,  16.86330209,  11.62256169],\n",
       "         [  8.43980527,  18.8413549 ,  13.26440228],\n",
       "         [  9.14046311,  21.2670231 ,  15.36957294],\n",
       "         [ 10.06142082,  24.29336475,  18.03866059],\n",
       "         [ 11.22666359,  28.06656522,  21.41920566],\n",
       "         [ 12.72266063,  32.78151862,  25.70654585],\n",
       "         [ 14.71397048,  38.64177467,  31.30106224],\n",
       "         [ 17.30345409,  46.18543574,  38.56031085],\n",
       "         [ 20.64598938,  55.96543134,  47.99851556],\n",
       "         [ 24.98711782,  68.71603535,  60.09733366],\n",
       "         [ 30.52505609,  85.38007162,  75.93418074],\n",
       "         [ 37.87948934, 106.92020655,  96.69155145],\n",
       "         [ 47.06305988, 134.88362178, 123.60840852],\n",
       "         [ 58.41870661, 172.15038881, 159.14411798],\n",
       "         [ 73.37743162, 220.31268029, 206.16731344],\n",
       "         [ 92.3592575 , 284.54215568, 268.86026438],\n",
       "         [116.39528868, 372.29230198, 353.89589628],\n",
       "         [146.28630205, 489.75449762, 466.38817052]]),\n",
       "  'FRA_model': array([[ 3.51785781,  7.87699325,  9.63448341],\n",
       "         [ 3.66769407,  8.33717297,  9.97039827],\n",
       "         [ 3.88894195,  8.97617185, 10.50897005],\n",
       "         [ 4.17404376,  9.81138264, 11.27172325],\n",
       "         [ 4.54403117, 10.89046094, 12.29826668],\n",
       "         [ 5.02890641, 12.25551824, 13.62867815],\n",
       "         [ 5.65271823, 14.00650267, 15.340467  ],\n",
       "         [ 6.46524852, 16.26602287, 17.53258749],\n",
       "         [ 7.48101902, 19.15068931, 20.37940611],\n",
       "         [ 8.85233755, 22.82612809, 24.08986988],\n",
       "         [10.6250746 , 27.57061268, 28.88167187],\n",
       "         [12.93152929, 33.73775762, 35.10814004]]),\n",
       "  'GBR_model': array([[  7.44183107,   5.61744605,   7.29032561],\n",
       "         [  7.78223253,   5.31923138,   7.60464444],\n",
       "         [  8.22008859,   5.08921792,   8.03522195],\n",
       "         [  8.76336775,   4.93308616,   8.57378244],\n",
       "         [  9.4297868 ,   4.85292516,   9.24861718],\n",
       "         [ 10.25572441,   4.85599005,  10.08647768],\n",
       "         [ 11.25536613,   4.92713144,  11.11938118],\n",
       "         [ 12.47776721,   5.09509353,  12.40580422],\n",
       "         [ 13.98970879,   5.36900236,  14.00589364],\n",
       "         [ 15.8343092 ,   5.73018829,  16.00669417],\n",
       "         [ 18.09017859,   6.22787946,  18.50547721],\n",
       "         [ 20.85540891,   6.83482269,  21.65181933],\n",
       "         [ 24.29614285,   7.65509243,  25.59701732],\n",
       "         [ 28.62782807,   8.64899977,  30.5974402 ],\n",
       "         [ 34.02400735,   9.87846721,  36.99423175],\n",
       "         [ 40.86751922,  11.45286027,  45.27302995],\n",
       "         [ 49.56692088,  13.41261153,  55.93007257],\n",
       "         [ 60.4514421 ,  15.81330959,  69.83255419],\n",
       "         [ 74.62213149,  18.62550064,  87.9679973 ],\n",
       "         [ 93.46226484,  22.36491564, 111.85830497],\n",
       "         [117.89993519,  26.52120119, 143.43452139],\n",
       "         [149.8425794 ,  30.78723144, 186.10059804]])},\n",
       " {'GBR_model': array([6.97268305, 6.37678414, 7.        ])})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_year(1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoch, 100000):\n",
    "    fd = {current_year:3000}\n",
    "    for step in range(100):\n",
    "        sess.run(main_op, fd)\n",
    "    s, _ = sess.run([summary, main_op], fd)\n",
    "    writer.add_summary(s, global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_post=global_inf.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "for _ in range(10000):\n",
    "    ss.append(indivs['FRA'].eval())\n",
    "ss = np.array(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ss,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ss,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ss,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ss[:,1], ss[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess,'/tmp/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (sys p)",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
